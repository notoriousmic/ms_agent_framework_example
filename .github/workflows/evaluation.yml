name: Evaluate Multi-Agent System

on:
  push:
    branches: [ "main", "master" ]
  schedule:
    # Run evaluation daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub Actions UI
jobs:
  evaluate:
    runs-on: ubuntu-latest
    environment: ms_agent_framework
    env:
      AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
      AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
      AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
      AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME: ${{ secrets.AZURE_OPENAI_RESPONSES_DEPLOYMENT_NAME }}
      AZURE_AI_PROJECT_ENDPOINT: ${{ secrets.AZURE_AI_PROJECT_ENDPOINT }}
      AZURE_AI_MODEL_DEPLOYMENT_NAME: ${{ secrets.AZURE_AI_MODEL_DEPLOYMENT_NAME }}
      PROJECT_ENDPOINT: ${{ secrets.PROJECT_ENDPOINT }}
      AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
      AZURE_RESOURCE_GROUP: ${{ secrets.AZURE_RESOURCE_GROUP }}
      AZURE_PROJECT_NAME: ${{ secrets.AZURE_PROJECT_NAME }}
      BRAVE_API_KEY: ${{ secrets.BRAVE_API_KEY }}
      ENABLE_OTEL: ${{ vars.ENABLE_OTEL || 'true' }}
      ENABLE_SENSITIVE_DATA: ${{ vars.ENABLE_SENSITIVE_DATA || 'false' }}
      APPLICATIONINSIGHTS_CONNECTION_STRING: ${{ secrets.APPLICATIONINSIGHTS_CONNECTION_STRING }}
      AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
      AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}
      AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
      SKIP_AGENT_CREATION: "true"
    steps:
      - uses: actions/checkout@v4

      - name: Install uv and Python 3.12
        uses: astral-sh/setup-uv@v5
        with:
          python-version: '3.12'
          enable-cache: true
          cache-dependency-glob: uv.lock

      - name: Install the project
        run: uv sync --locked --all-extras
      - name: Run Multi-Agent System Evaluation
        env:
          PYTHONPATH: ${{ github.workspace }}/src
        run: |
          echo "Starting evaluation of multi-agent system..."
          uv run python src/microsoft_agent_framework/application/evaluation_service/eval.py

      - name: Check Evaluation Scores Against Quality Gates
        if: always()
        id: quality_check
        run: |
          python .github/scripts/check_evaluation_scores.py data/evaluation_results.jsonl --config .github/evaluation-config.json
        continue-on-error: true

      - name: Post Quality Gate Status
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const passed = "${{ steps.quality_check.outcome }}" === "success";
            const state = passed ? "success" : "failure";
            const description = passed
              ? "âœ… Evaluation quality gates passed"
              : "âŒ Evaluation quality gates failed - see logs for details";

            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              description: description,
              context: "Quality Gate Check / Evaluation"
            });

      - name: Upload Evaluation Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: data/evaluation_results.jsonl
          retention-days: 30

      - name: Comment PR with Evaluation Summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('data/evaluation_results.jsonl')) {
              const results = fs.readFileSync('data/evaluation_results.jsonl', 'utf8')
                .trim()
                .split('\n')
                .map(line => JSON.parse(line));

              const qualityCheckPassed = "${{ steps.quality_check.outcome }}" === "success";
              const qualityIcon = qualityCheckPassed ? "âœ…" : "âŒ";
              const qualityStatus = qualityCheckPassed
                ? "**PASSED** - All quality gates met"
                : "**FAILED** - Some quality gates not met (see logs)";

              const summary = `## ðŸ“Š Multi-Agent System Evaluation Results

              **Total Queries Evaluated**: ${results.length}
              **Quality Gate Status**: ${qualityIcon} ${qualityStatus}

              ### Evaluation Metrics
              - **Status**: Evaluation completed successfully
              - **Portal**: View detailed results in [Azure AI Foundry Portal](https://ai.azure.com)
              - **Configuration**: Customize thresholds in [evaluation-config.json](.github/evaluation-config.json)

              ### Test Queries Processed
              ${results.map((r, i) => `${i + 1}. ${r.query?.substring(0, 60)}...`).join('\n')}

              Results have been submitted to Azure AI Foundry for cloud evaluation using Groundedness and Relevance metrics.

              ### Quality Gate Details
              See the "Check Evaluation Scores Against Quality Gates" step in the workflow logs for detailed score breakdowns.
              `;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            }
